{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "8SjtP8vW2HCG"
   },
   "outputs": [],
   "source": [
    "#! pip install keras\n",
    "#! pip install --user tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "gdnS9kU22HCK"
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "#from keras.preprocessing import sequence\n",
    "from keras.utils.data_utils import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D\n",
    "from keras.datasets import imdb #imdb is a movie review dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "-Czc_pAa2HCK"
   },
   "outputs": [],
   "source": [
    "# set parameters:\n",
    "max_features = 5000\n",
    "maxlen = 400\n",
    "batch_size = 32\n",
    "embedding_dims = 50\n",
    "filters = 250\n",
    "kernel_size = 3\n",
    "hidden_dims = 250\n",
    "epochs = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "kN74Xezm2HCL"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#use below, if it gives a imdb load error\n",
    "#np_load_old = np.load  # save old function for calling later\n",
    "# modify the default parameters of np.load\n",
    "#np.load = lambda *a,**k: np_load_old(*a, allow_pickle = True, **k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TVrXkFHU2HCL",
    "outputId": "804f1392-543d-42ba-d44b-b4771b552a00"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "25000 train sequences\n",
      "25000 test sequences\n"
     ]
    }
   ],
   "source": [
    "print('Loading data...')\n",
    "\n",
    "(x_train, y_train),(x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k2CokA9E2HCM"
   },
   "source": [
    "# Understanding the inputs to the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y7FAbHp_2HCN"
   },
   "source": [
    "One of the most difficult things to do is to feed your own data to the model. That is extremely important if you want to apply your skills in the real world -- to actually put machine learning to use in the industry.\n",
    "\n",
    "Let's try to understand our inputs here so that we can later feed our own data to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UqU3nouh2HCO",
    "outputId": "d267b4ad-a61a-4d94-f3f5-cdcd9bc8621b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000,)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape  # 25000 samples, each with a total (fixed) length of 400 'words'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sBuM4fhN2HCP",
    "outputId": "4295839a-f755-481f-97a3-9674a2777d01"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000,)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Bo6L9r5B2HCP",
    "outputId": "835364cc-4e66-4c29-af7b-26dc82c8a4a8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0]      # labels are easy to understand. 0 is negative sentiment, 1 is positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VJdtwh9O2HCQ",
    "outputId": "ed5b6586-4e5e-4e56-a24d-eff56da992f3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 14,\n",
       " 22,\n",
       " 16,\n",
       " 43,\n",
       " 530,\n",
       " 973,\n",
       " 1622,\n",
       " 1385,\n",
       " 65,\n",
       " 458,\n",
       " 4468,\n",
       " 66,\n",
       " 3941,\n",
       " 4,\n",
       " 173,\n",
       " 36,\n",
       " 256,\n",
       " 5,\n",
       " 25,\n",
       " 100,\n",
       " 43,\n",
       " 838,\n",
       " 112,\n",
       " 50,\n",
       " 670,\n",
       " 2,\n",
       " 9,\n",
       " 35,\n",
       " 480,\n",
       " 284,\n",
       " 5,\n",
       " 150,\n",
       " 4,\n",
       " 172,\n",
       " 112,\n",
       " 167,\n",
       " 2,\n",
       " 336,\n",
       " 385,\n",
       " 39,\n",
       " 4,\n",
       " 172,\n",
       " 4536,\n",
       " 1111,\n",
       " 17,\n",
       " 546,\n",
       " 38,\n",
       " 13,\n",
       " 447,\n",
       " 4,\n",
       " 192,\n",
       " 50,\n",
       " 16,\n",
       " 6,\n",
       " 147,\n",
       " 2025,\n",
       " 19,\n",
       " 14,\n",
       " 22,\n",
       " 4,\n",
       " 1920,\n",
       " 4613,\n",
       " 469,\n",
       " 4,\n",
       " 22,\n",
       " 71,\n",
       " 87,\n",
       " 12,\n",
       " 16,\n",
       " 43,\n",
       " 530,\n",
       " 38,\n",
       " 76,\n",
       " 15,\n",
       " 13,\n",
       " 1247,\n",
       " 4,\n",
       " 22,\n",
       " 17,\n",
       " 515,\n",
       " 17,\n",
       " 12,\n",
       " 16,\n",
       " 626,\n",
       " 18,\n",
       " 2,\n",
       " 5,\n",
       " 62,\n",
       " 386,\n",
       " 12,\n",
       " 8,\n",
       " 316,\n",
       " 8,\n",
       " 106,\n",
       " 5,\n",
       " 4,\n",
       " 2223,\n",
       " 2,\n",
       " 16,\n",
       " 480,\n",
       " 66,\n",
       " 3785,\n",
       " 33,\n",
       " 4,\n",
       " 130,\n",
       " 12,\n",
       " 16,\n",
       " 38,\n",
       " 619,\n",
       " 5,\n",
       " 25,\n",
       " 124,\n",
       " 51,\n",
       " 36,\n",
       " 135,\n",
       " 48,\n",
       " 25,\n",
       " 1415,\n",
       " 33,\n",
       " 6,\n",
       " 22,\n",
       " 12,\n",
       " 215,\n",
       " 28,\n",
       " 77,\n",
       " 52,\n",
       " 5,\n",
       " 14,\n",
       " 407,\n",
       " 16,\n",
       " 82,\n",
       " 2,\n",
       " 8,\n",
       " 4,\n",
       " 107,\n",
       " 117,\n",
       " 2,\n",
       " 15,\n",
       " 256,\n",
       " 4,\n",
       " 2,\n",
       " 7,\n",
       " 3766,\n",
       " 5,\n",
       " 723,\n",
       " 36,\n",
       " 71,\n",
       " 43,\n",
       " 530,\n",
       " 476,\n",
       " 26,\n",
       " 400,\n",
       " 317,\n",
       " 46,\n",
       " 7,\n",
       " 4,\n",
       " 2,\n",
       " 1029,\n",
       " 13,\n",
       " 104,\n",
       " 88,\n",
       " 4,\n",
       " 381,\n",
       " 15,\n",
       " 297,\n",
       " 98,\n",
       " 32,\n",
       " 2071,\n",
       " 56,\n",
       " 26,\n",
       " 141,\n",
       " 6,\n",
       " 194,\n",
       " 2,\n",
       " 18,\n",
       " 4,\n",
       " 226,\n",
       " 22,\n",
       " 21,\n",
       " 134,\n",
       " 476,\n",
       " 26,\n",
       " 480,\n",
       " 5,\n",
       " 144,\n",
       " 30,\n",
       " 2,\n",
       " 18,\n",
       " 51,\n",
       " 36,\n",
       " 28,\n",
       " 224,\n",
       " 92,\n",
       " 25,\n",
       " 104,\n",
       " 4,\n",
       " 226,\n",
       " 65,\n",
       " 16,\n",
       " 38,\n",
       " 1334,\n",
       " 88,\n",
       " 12,\n",
       " 16,\n",
       " 283,\n",
       " 5,\n",
       " 16,\n",
       " 4472,\n",
       " 113,\n",
       " 103,\n",
       " 32,\n",
       " 15,\n",
       " 16,\n",
       " 2,\n",
       " 19,\n",
       " 178,\n",
       " 32]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0] #the actual reviews, each correspond to a word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HSGDUstH2HCQ",
    "outputId": "c92e101b-c2ee-4430-ce26-f3ff8438995b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('fawn', 34701),\n",
       " ('tsukino', 52006),\n",
       " ('nunnery', 52007),\n",
       " ('sonja', 16816),\n",
       " ('vani', 63951),\n",
       " ('woods', 1408),\n",
       " ('spiders', 16115),\n",
       " ('hanging', 2345),\n",
       " ('woody', 2289),\n",
       " ('trawling', 52008)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#lets see what is going on in the actual dataset\n",
    "word_to_id = keras.datasets.imdb.get_word_index()\n",
    "list(word_to_id.items())[:10] #key value pairs in the dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-au5VLbl2HCQ",
    "outputId": "95d7b984-24dc-43df-ab31-323ddc9289fc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "116\n",
      "37\n",
      "354\n",
      "218\n"
     ]
    }
   ],
   "source": [
    "#see numbers correspodnign to each specific word, mapping from words to numbers\n",
    "print(word_to_id['love'])\n",
    "print(word_to_id['like'])\n",
    "print(word_to_id['boring'])\n",
    "print(word_to_id['interesting'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "uXB0bp7m2HCQ"
   },
   "outputs": [],
   "source": [
    "# goal is to go from a sentence to sequence of numbers (for our future dataset it is important)\n",
    "def get_fixed_word_to_id_dict():\n",
    "    INDEX_FROM=3   # word index offset\n",
    "\n",
    "    word_to_id = keras.datasets.imdb.get_word_index()\n",
    "    word_to_id = {k:(v+INDEX_FROM) for k,v in word_to_id.items()}\n",
    "    word_to_id[\" \"] = 0\n",
    "    word_to_id[\"<START>\"] = 1\n",
    "    word_to_id[\"<UNK>\"] = 2 #unkwoen word\n",
    "    return word_to_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "WQibLFwW2HCQ"
   },
   "outputs": [],
   "source": [
    "def decode_to_sentence(data_point):\n",
    "    #NUM_WORDS=1000 # only use top 1000 words\n",
    "\n",
    "    word_to_id = get_fixed_word_to_id_dict()\n",
    "\n",
    "    id_to_word = {value:key for key,value in word_to_id.items()}\n",
    "    return ' '.join( id_to_word[id] for id in data_point )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "j-oAERBl2HCQ"
   },
   "outputs": [],
   "source": [
    "data_point_to_show = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XfD8JsVH2HCR",
    "outputId": "c8d93770-4a50-4dcf-dcde-0be1e534e5f8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 14,\n",
       " 22,\n",
       " 16,\n",
       " 43,\n",
       " 530,\n",
       " 973,\n",
       " 1622,\n",
       " 1385,\n",
       " 65,\n",
       " 458,\n",
       " 4468,\n",
       " 66,\n",
       " 3941,\n",
       " 4,\n",
       " 173,\n",
       " 36,\n",
       " 256,\n",
       " 5,\n",
       " 25,\n",
       " 100,\n",
       " 43,\n",
       " 838,\n",
       " 112,\n",
       " 50,\n",
       " 670,\n",
       " 2,\n",
       " 9,\n",
       " 35,\n",
       " 480,\n",
       " 284,\n",
       " 5,\n",
       " 150,\n",
       " 4,\n",
       " 172,\n",
       " 112,\n",
       " 167,\n",
       " 2,\n",
       " 336,\n",
       " 385,\n",
       " 39,\n",
       " 4,\n",
       " 172,\n",
       " 4536,\n",
       " 1111,\n",
       " 17,\n",
       " 546,\n",
       " 38,\n",
       " 13,\n",
       " 447,\n",
       " 4,\n",
       " 192,\n",
       " 50,\n",
       " 16,\n",
       " 6,\n",
       " 147,\n",
       " 2025,\n",
       " 19,\n",
       " 14,\n",
       " 22,\n",
       " 4,\n",
       " 1920,\n",
       " 4613,\n",
       " 469,\n",
       " 4,\n",
       " 22,\n",
       " 71,\n",
       " 87,\n",
       " 12,\n",
       " 16,\n",
       " 43,\n",
       " 530,\n",
       " 38,\n",
       " 76,\n",
       " 15,\n",
       " 13,\n",
       " 1247,\n",
       " 4,\n",
       " 22,\n",
       " 17,\n",
       " 515,\n",
       " 17,\n",
       " 12,\n",
       " 16,\n",
       " 626,\n",
       " 18,\n",
       " 2,\n",
       " 5,\n",
       " 62,\n",
       " 386,\n",
       " 12,\n",
       " 8,\n",
       " 316,\n",
       " 8,\n",
       " 106,\n",
       " 5,\n",
       " 4,\n",
       " 2223,\n",
       " 2,\n",
       " 16,\n",
       " 480,\n",
       " 66,\n",
       " 3785,\n",
       " 33,\n",
       " 4,\n",
       " 130,\n",
       " 12,\n",
       " 16,\n",
       " 38,\n",
       " 619,\n",
       " 5,\n",
       " 25,\n",
       " 124,\n",
       " 51,\n",
       " 36,\n",
       " 135,\n",
       " 48,\n",
       " 25,\n",
       " 1415,\n",
       " 33,\n",
       " 6,\n",
       " 22,\n",
       " 12,\n",
       " 215,\n",
       " 28,\n",
       " 77,\n",
       " 52,\n",
       " 5,\n",
       " 14,\n",
       " 407,\n",
       " 16,\n",
       " 82,\n",
       " 2,\n",
       " 8,\n",
       " 4,\n",
       " 107,\n",
       " 117,\n",
       " 2,\n",
       " 15,\n",
       " 256,\n",
       " 4,\n",
       " 2,\n",
       " 7,\n",
       " 3766,\n",
       " 5,\n",
       " 723,\n",
       " 36,\n",
       " 71,\n",
       " 43,\n",
       " 530,\n",
       " 476,\n",
       " 26,\n",
       " 400,\n",
       " 317,\n",
       " 46,\n",
       " 7,\n",
       " 4,\n",
       " 2,\n",
       " 1029,\n",
       " 13,\n",
       " 104,\n",
       " 88,\n",
       " 4,\n",
       " 381,\n",
       " 15,\n",
       " 297,\n",
       " 98,\n",
       " 32,\n",
       " 2071,\n",
       " 56,\n",
       " 26,\n",
       " 141,\n",
       " 6,\n",
       " 194,\n",
       " 2,\n",
       " 18,\n",
       " 4,\n",
       " 226,\n",
       " 22,\n",
       " 21,\n",
       " 134,\n",
       " 476,\n",
       " 26,\n",
       " 480,\n",
       " 5,\n",
       " 144,\n",
       " 30,\n",
       " 2,\n",
       " 18,\n",
       " 51,\n",
       " 36,\n",
       " 28,\n",
       " 224,\n",
       " 92,\n",
       " 25,\n",
       " 104,\n",
       " 4,\n",
       " 226,\n",
       " 65,\n",
       " 16,\n",
       " 38,\n",
       " 1334,\n",
       " 88,\n",
       " 12,\n",
       " 16,\n",
       " 283,\n",
       " 5,\n",
       " 16,\n",
       " 4472,\n",
       " 113,\n",
       " 103,\n",
       " 32,\n",
       " 15,\n",
       " 16,\n",
       " 2,\n",
       " 19,\n",
       " 178,\n",
       " 32]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[data_point_to_show]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "92fiehWS2HCR",
    "outputId": "2003ae46-8272-42bc-d478-66147ec33d18"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<START> this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert <UNK> is an amazing actor and now the same being director <UNK> father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for <UNK> and would recommend it to everyone to watch and the fly <UNK> was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also <UNK> to the two little <UNK> that played the <UNK> of norman and paul they were just brilliant children are often left out of the <UNK> list i think because the stars that play them all grown up are such a big <UNK> for the whole film but these children are amazing and should be <UNK> for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was <UNK> with us all\n"
     ]
    }
   ],
   "source": [
    "print(decode_to_sentence(x_train[data_point_to_show]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jqz_Dsht2HCR",
    "outputId": "bce3f146-e689-4b9f-bba6-c5f1959f1571"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(y_train[data_point_to_show]) # to see the actual sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "pNupzg-T2HCR"
   },
   "outputs": [],
   "source": [
    "def encode_sentence(sent):\n",
    "    # print(sent)\n",
    "    encoded = []\n",
    "\n",
    "    word_to_id = get_fixed_word_to_id_dict()\n",
    "\n",
    "    for w in sent.split(\" \"):\n",
    "        if w in word_to_id:\n",
    "            encoded.append(word_to_id[w])\n",
    "        else:\n",
    "            encoded.append(2)        # We used '2' for <UNK>\n",
    "    return encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NRNblx6_2HCR",
    "outputId": "c3dbf702-a8a8-480c-f129-de35b98bb79a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[34704, 16819, 63954, 2]\n",
      "[14, 127, 24, 168, 52]\n"
     ]
    }
   ],
   "source": [
    "words = \"fawn sonja vani made-up-word\"\n",
    "print(encode_sentence(words))\n",
    "print(encode_sentence(\"this does not look good\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GEo83x-M2HCR",
    "outputId": "28dce59d-d146-4361-c688-30b07a912101"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pad sequences (samples x time)\n",
      "x_train shape: (25000, 400)\n",
      "x_test shape: (25000, 400)\n"
     ]
    }
   ],
   "source": [
    "#Very Important Part!\n",
    "#reviews have diff lengths, how we can build an input function that give it a variable sequence\n",
    "#max-length would be good way, and anything shorter that we Pad (insert zeros at the beginning)\n",
    "print('Pad sequences (samples x time)')\n",
    "x_train = pad_sequences(x_train, maxlen=maxlen)#e.g. for Twitter we already know that\n",
    "x_test = pad_sequences(x_test, maxlen=maxlen)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "G4Ik7Jvg2HCS",
    "outputId": "b9025b93-919b-4ec4-a0d8-ec7a427ef3b1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    1   14   22   16   43  530  973 1622 1385   65  458 4468   66 3941\n",
      "    4  173   36  256    5   25  100   43  838  112   50  670    2    9\n",
      "   35  480  284    5  150    4  172  112  167    2  336  385   39    4\n",
      "  172 4536 1111   17  546   38   13  447    4  192   50   16    6  147\n",
      " 2025   19   14   22    4 1920 4613  469    4   22   71   87   12   16\n",
      "   43  530   38   76   15   13 1247    4   22   17  515   17   12   16\n",
      "  626   18    2    5   62  386   12    8  316    8  106    5    4 2223\n",
      "    2   16  480   66 3785   33    4  130   12   16   38  619    5   25\n",
      "  124   51   36  135   48   25 1415   33    6   22   12  215   28   77\n",
      "   52    5   14  407   16   82    2    8    4  107  117    2   15  256\n",
      "    4    2    7 3766    5  723   36   71   43  530  476   26  400  317\n",
      "   46    7    4    2 1029   13  104   88    4  381   15  297   98   32\n",
      " 2071   56   26  141    6  194    2   18    4  226   22   21  134  476\n",
      "   26  480    5  144   30    2   18   51   36   28  224   92   25  104\n",
      "    4  226   65   16   38 1334   88   12   16  283    5   16 4472  113\n",
      "  103   32   15   16    2   19  178   32]\n"
     ]
    }
   ],
   "source": [
    "print(x_train[0]) #following matrix gives us words, 12 is not smaller than 20! diff words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-4XXHfv-2HCS",
    "outputId": "acf57819-3e2f-4e96-f3bc-55d5b86aefe6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n",
      "Epoch 1/2\n",
      "782/782 [==============================] - 27s 34ms/step - loss: 0.3992 - accuracy: 0.8041 - val_loss: 0.2717 - val_accuracy: 0.8849\n",
      "Epoch 2/2\n",
      "782/782 [==============================] - 28s 35ms/step - loss: 0.2309 - accuracy: 0.9064 - val_loss: 0.2646 - val_accuracy: 0.8914\n",
      "782/782 [==============================] - 5s 7ms/step - loss: 0.2646 - accuracy: 0.8914\n",
      "Test score: 0.26461467146873474\n",
      "Test accuracy: 0.8914399743080139\n"
     ]
    }
   ],
   "source": [
    "print('Build model...')\n",
    "model = Sequential()\n",
    "\n",
    "# we start off with an efficient embedding layer which maps\n",
    "# our vocab indices into embedding_dims dimensions\n",
    "#the problem of fake ordering, machine may assume numbers presnet first are more important\n",
    "#we represent word-id as 2D vectors to address the above issue\n",
    "#read more here https://nlp.stanford.edu/projects/glove/\n",
    "#embedding is the representaion in vector\n",
    "model.add(Embedding(max_features,\n",
    "                    embedding_dims,\n",
    "                    input_length=maxlen))\n",
    "\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# we add a Convolution1D, which will learn filters\n",
    "# word group filters of size filter_length:\n",
    "model.add(Conv1D(filters,\n",
    "                 kernel_size,\n",
    "                 padding='valid',\n",
    "                 activation='relu',\n",
    "                 strides=1))\n",
    "# we use max pooling:\n",
    "model.add(GlobalMaxPooling1D())\n",
    "\n",
    "# We add a vanilla hidden layer:\n",
    "model.add(Dense(hidden_dims))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "# We project onto a single unit output layer, and squash it with a sigmoid:\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_data=(x_test, y_test))\n",
    "\n",
    "score, acc = model.evaluate(x_test, y_test,\n",
    "                            batch_size=batch_size)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aEpW515I2HCS",
    "outputId": "a03ddf26-760e-4e0c-9426-b6bc2c42f30d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    1,  591,  202,   14,   31,    6,  717,   10,   10,\n",
       "          2,    2,    5,    4,  360,    7,    4,  177,    2,  394,  354,\n",
       "          4,  123,    9, 1035, 1035, 1035,   10,   10,   13,   92,  124,\n",
       "         89,  488,    2,  100,   28, 1668,   14,   31,   23,   27,    2,\n",
       "         29,  220,  468,    8,  124,   14,  286,  170,    8,  157,   46,\n",
       "          5,   27,  239,   16,  179,    2,   38,   32,   25,    2,  451,\n",
       "        202,   14,    6,  717])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NgHA_Yss2HCS",
    "outputId": "b12489c2-03ae-4e20-f2d9-48524d3e2611"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 6s 7ms/step\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(x_test)\n",
    "sentiment = ['NEG' if i < 0.5 else 'POS' for i in predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "J0W7SPlD2HCS",
    "outputId": "c298d36c-ebcd-4b1d-8a6f-a8c14f354563"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                                                                                                                        <START> this film requires a lot of <UNK> because it focuses on mood and character development the plot is very simple and many of the scenes take place on the same set in <UNK> <UNK> the <UNK> dennis character apartment but the film builds to a disturbing climax br br the characters create an atmosphere <UNK> with sexual tension and psychological <UNK> it's very interesting that robert <UNK> directed this considering the style and structure of his other films still the <UNK> <UNK> audio style is evident here and there i think what really makes this film work is the brilliant performance by <UNK> dennis it's definitely one of her darker characters but she plays it so perfectly and convincingly that it's scary michael burns does a good job as the <UNK> young man regular <UNK> player michael murphy has a small part the <UNK> moody set fits the content of the story very well in short this movie is a powerful study of <UNK> sexual <UNK> and desperation be patient <UNK> up the atmosphere and pay attention to the wonderfully written script br br i praise robert <UNK> this is one of his many films that deals with <UNK> fascinating subject matter this film is disturbing but it's sincere and it's sure to <UNK> a strong emotional response from the viewer if you want to see an unusual film some might even say bizarre this is worth the time br br unfortunately it's very difficult to find in video <UNK> you may have to buy it off the internet -- POS\n"
     ]
    }
   ],
   "source": [
    "data_point_to_show = 1\n",
    "print(decode_to_sentence(x_test[data_point_to_show]), \"--\", sentiment[data_point_to_show])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "FIk2oWRI2HCS"
   },
   "outputs": [],
   "source": [
    "test_sentences = []\n",
    "\n",
    "test_sentence = \"i do not like this at all\"\n",
    "test_sentence = encode_sentence(test_sentence)\n",
    "test_sentences.append(test_sentence)\n",
    "\n",
    "\n",
    "test_sentence = \"loved it\"\n",
    "test_sentence = encode_sentence(test_sentence)\n",
    "test_sentences.append(test_sentence)\n",
    "\n",
    "\n",
    "test_sentence = \"did not love it\"\n",
    "test_sentence = encode_sentence(test_sentence)\n",
    "test_sentences.append(test_sentence)\n",
    "\n",
    "\n",
    "test_sentence = \"cannot say that i loved it\"\n",
    "test_sentence = encode_sentence(test_sentence)\n",
    "test_sentences.append(test_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "Kja2BqJi2HCT"
   },
   "outputs": [],
   "source": [
    "test_sentences = pad_sequences(test_sentences, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ek6YcjhH2HCT",
    "outputId": "1e60763c-818c-465b-b81a-76c016f40901"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 400)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sentences.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2T_Hrct-2HCT",
    "outputId": "d2eb45c5-5810-412c-ebf5-beaa1c63bd8d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 16ms/step\n",
      "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  i do not like this at all -- NEG\n",
      "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            loved it -- POS\n",
      "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        did not love it -- NEG\n",
      "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    cannot say that i loved it -- POS\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(test_sentences)\n",
    "sentiment = ['NEG' if i < 0.5 else 'POS' for i in predictions]\n",
    "\n",
    "for i in range(test_sentences.shape[0]):\n",
    "    print(decode_to_sentence(test_sentences[i]), \"--\", sentiment[i])\n",
    "#Isn't the last one correct?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0cFesCh62HCT"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
